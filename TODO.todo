Controllare e sistemare lo pseudo codice con tutto aggiornato e magari guardare come è fatto quello BDI standard per confrontare.

Testare:
- User provided deve saltare generazione del desire e evaluation del desire (richiedere all'utente) e generazione della trigger function
- Stateless intention generation devo solo modificare la funzione manager.get_library() per includere solo le azioni base
- No desire triggering semplicemente non chiamo la prima parte in nel loop in control
- A quanto pare la mia idea che il desire può essere eseguito utilizzando tutte le intenzioni che contiene è sbagliata, sembrerebbe che sia sufficiente eseguire solamente l'ultima intenzione che scrive, cambiato in run_desire nel manager
- desire implementati listati nella control question 1
- desire trigger evaluation se è positiva o negativa per supportare l'ambiente che cambia nel tempo

Aggiungere desire implementati (funzionanti) alla desire generation control question 1, così da evitare che generi sempre lo stesso desire. Nel caso l'ambiente cambiasse può essere che non diano errore e che solamente il piano generato non raggiunga più l'effetto desiderato. Per quel caso devo aggiungere un altro prompt che chiede alla LLM se un desire appena eseguito (già generato, e quindi triggerato soltanto) ha portato al risultato aspettato. E in questo modo posso controllare che continuino ad avere senso anche se l'ambiente cambia. Per questo prompt posso usare la control question 5 che è quella usata per verificare se un nuovo desire funziona.


Nel caso l'ambiente cambiasse alcune intention potrebbero non funzionare più. Se danno errore allora vengono eliminate dalla libreria, ma se solo generano un piano che non funziona più allora deve essere la LLM ad accorgersi e segnarsi di non utilizzare quella intention. Per quanto riguarda invece le intention già implementate devo avere il meccanismo della memoria che mi dice di non utilizzarla, nel caso una intention non dia errore ma semplicemente generi un piano che no porta al risultato previsto quando la funzione era stata scritta.



Adattare descrizioni delle azioni base per renderle "This function ..." e poi continuare con la descrizione, e mettere le informazioni sull'ambiente nel contesto.

Scrivere meglio l'evento della mappa aggiugendo le coordinate per ogni cella.



Creare stima di costo per esperimento?



Implementare memoria? In questo caso tolgo la descrizione delle azioni base, tolgo le informazioni dal contesto, e permetto alla LLM di scrivere delle frasi su cosa ha imparato riguardo il belief set, le azioni implementate, come interagiscono con il mondo, gli eventi che riceve. Questo step potrebbe essere inserito nell'intention evaluation, però magari da staccare senza includere l'intention e solo dando il belief set iniziale e le azioni compiute con i loro eventi. In questo modo però non so a quali azioni corrispondono le funzioni già implementate, quindi magari sarebbe da incitare la LLM in control question 2, o nel contesto, a scoprire inizialmente cosa fanno le azioni base.

Per avere i replay della finestra di pygame, potrei utilizzare seed per riprodurre le stesse generazioni di elementi casuali, e per le azioni invece posso salvare lato server le azioni che ricevo da ogni agente in quale frame sono state ricevute e poi nel replay simulare la loro ricezione, utilizzando già l'architettura che ho nel file server.py.


Idealmente vorrei un ambiente con pochi elementi, per evitare di creare complessità inutile, e per tenere la lunghezza dei prompt bassa. Quindi mappa piccola, basso spawn rate delle parcelle, decay lento o nullo, magari con punteggio basso delle parcelle.

Provare ancora a migliorare la descrizione che fornisce delle intention prodotte. Ad esempio istruendo la LLM di iniziare con "This function ..." e poi continuare con la descrizione. In questo caso dovrei anche cambiare la descrizione delle azioni base. E in questo caso magari stacco le informazioni dell'ambiente e le metto in una sezione a parte del prompt.

Rivedere desire evaluation e aggiungere altre informazioni, magari riguardanti le intention eseguite, o solo l'ultima con la sua descrizione?




Provare a cambiare la temperature per la request cambia le cose.

Cambiare nel prompting il self.stop quando pronto per esperimenti.

Per poter runnare il progetto serve: pygame, astor, python-dotenv, PyQt5, openai e ovviamente il file .env con le variablili di ambiente. Aggiornare magari la descrizione della repo così da spiegare come farlo partire.

https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/


Cose di cui parlare:
- a che punto sono con il progetto
- nuovo meccanismo della memoria dato dal problema della conoscenza dell'utente sulle azioni e come modificano l'ambiente (ed eventi)
- come eseguire gli esperimenti
- mostrare esempio di esperimento
- modello di azure

Note su incontro:
evoluzione dell'ambiente con cambiamenti della mappa, o aggiunta di nuovi elementi

provare senza score, o con delivery zones che danno più punti

capire se ci possono essere evoluzioni diverse con lo stesso gioco, capire se ci sono diverse direzioni di evoluzione

reward negativo per azioni sbagliate