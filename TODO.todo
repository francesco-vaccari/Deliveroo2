Controllare e sistemare lo pseudo codice con tutto aggiornato e magari guardare come è fatto quello BDI standard per confrontare.

Testare:
- User provided deve saltare generazione del desire e evaluation del desire (richiedere all'utente) e generazione della trigger function
- Stateless intention generation devo solo modificare la funzione manager.get_library() per includere solo le azioni base
- No desire triggering semplicemente non chiamo la prima parte in nel loop in control
- A quanto pare la mia idea che il desire può essere eseguito utilizzando tutte le intenzioni che contiene è sbagliata, sembrerebbe che sia sufficiente eseguire solamente l'ultima intenzione che scrive, cambiato in run_desire nel manager
- desire implementati listati nella control question 1
- desire trigger evaluation se è positiva o negativa per supportare l'ambiente che cambia nel tempo
- testare invalidazione di called intentions dopo 3 valutazioni negative (per gestire cambiamenti dell'ambiente)
- testare memoria, con nuove descrizioni delle azioni base, logging


Migliorare prompt per cosa inserire nella memoria?

Potrebbe essere interessante anche permettere alla LLM di modificare la memoria in ogni prompt così in step come valutazione delle intention o del desire, può aggiungere informazioni che servono negli step successivi per migliorare la generazione di desire e intention.

Forse dovrei specificare nel context prompt la pipeline di step che viene utilizzata, per guidare meglio che tipo di informazioni dovrebbero essere salvate nella memoria.



Creare stima di costo per esperimento?

Per avere i replay della finestra di pygame, potrei utilizzare seed per riprodurre le stesse generazioni di elementi casuali, e per le azioni invece posso salvare lato server le azioni che ricevo da ogni agente in quale frame sono state ricevute e poi nel replay simulare la loro ricezione, utilizzando già l'architettura che ho nel file server.py.

Idealmente vorrei un ambiente con pochi elementi, per evitare di creare complessità inutile, e per tenere la lunghezza dei prompt bassa. Quindi mappa piccola, basso spawn rate delle parcelle, decay lento o nullo, magari con punteggio basso delle parcelle.

Provare ancora a migliorare la descrizione che fornisce delle intention prodotte. Ad esempio istruendo la LLM di iniziare con "This function ..." e poi continuare con la descrizione. In questo caso dovrei anche cambiare la descrizione delle azioni base. E in questo caso magari stacco le informazioni dell'ambiente e le metto in una sezione a parte del prompt.

Rivedere desire evaluation e aggiungere altre informazioni, magari riguardanti le intention eseguite, o solo l'ultima con la sua descrizione?




Provare a cambiare la temperature per la request cambia le cose.

Cambiare nel prompting il self.stop quando pronto per esperimenti.

Per poter runnare il progetto serve: pygame, astor, python-dotenv, PyQt5, openai e ovviamente il file .env con le variablili di ambiente. Aggiornare magari la descrizione della repo così da spiegare come farlo partire.

https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/


Cose di cui parlare:
- a che punto sono con il progetto
- nuovo meccanismo della memoria dato dal problema della conoscenza dell'utente sulle azioni e come modificano l'ambiente (ed eventi)
- come eseguire gli esperimenti
- mostrare esempio di esperimento
- modello di azure

Note su incontro:
evoluzione dell'ambiente con cambiamenti della mappa, o aggiunta di nuovi elementi

provare senza score, o con delivery zones che danno più punti

capire se ci possono essere evoluzioni diverse con lo stesso gioco, capire se ci sono diverse direzioni di evoluzione

reward negativo per azioni sbagliate